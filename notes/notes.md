# Notes

Because I don't know anything about statistics/ML any more :(

## Englebrecht - Computational Intelligence

>   The firing of an artificial neuron (AN), and the strength of the exiting signal
    are controlled via a function, referred to as the activation function. The
    AN collects all inncoming siganls and computes a net input signal as a
    fucntion of the respective weights.

>   A nn is a realization of a nonlinear mapping from R^I to R^K, where I and K
    are, respectively, the dimension of the input and target (desired output)
    space. The function that does this is usually a complex function of a set
    of nonlinear functions, one for each neuron in the network.

*   Epoch - One iteration of training in which we've exhausted the training
    samples
*   Stochastic gradient descent - A version of gradient descent in which the
    partial derivatives of the cost functions with respect to the weights and
    biases are updated for only a random sample of the inputs.
*   mini-batch - a batch of training items to train on before updating the
    weights and biases
